{"cells":[{"cell_type":"markdown","metadata":{"id":"0Gjt_qmDvitn"},"source":["# Data Science 1 Tutorial 5.1 - Regression"]},{"cell_type":"markdown","metadata":{"id":"aGYOIU0pvitr"},"source":["In this tutorial, we will use the [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) dataset that is available from [Scikit-Learn](https://scikit-learn.org/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zgo2xBydvits"},"outputs":[],"source":["# Run this cell\n","from sklearn.datasets import fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"ugWBrqwfvitt"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZMDpw6Uvitu"},"outputs":[],"source":["# Complete the following\n","\n","# load the dataset into housing_data\n","housing_data = ________\n","\n","# Inspect the attributes in the output of the following\n","housing_data\n","\n","# What is the response variable? What is the dimension of the features?"]},{"cell_type":"markdown","metadata":{"id":"S42jOjH2vitv"},"source":["<u>Let's print only the description for easier reading. See from the output right above for the attribute that you need.</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akCSV8Ywvitv"},"outputs":[],"source":["# Print only the description\n","print(housing_data.________)"]},{"cell_type":"markdown","metadata":{"id":"fsEwTNmivitw"},"source":["<u>Now let's put the data into a DataFrame called housing_df.<br/>\n","You should have 8 predictive variables as given by `feature_names` plus the target variable, call this `medprice` for the medium price.</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"f1Og_KtAvitw"},"outputs":[],"source":["# Complete this as specified.\n","housing_df = pd.DataFrame(housing_data.________, #just the features\n","                          columns=housing_data.feature_names)\n","housing_df['medprice'] = pd.Series(housing_data.________)\n","\n","# print a few rows of housing_df\n","housing_df.head()"]},{"cell_type":"markdown","metadata":{"id":"KrTAXmtkvitx"},"source":["<u>For a quick exploration, display a pairplot of `housing_df`</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DP2T-WfTvitx"},"outputs":[],"source":["# Use pairplot from seaborn\n","sns.pairplot(housing_df)\n","#plt.savefig('housing.png')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"skyhmC6Avity"},"source":["## Simple Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"HigjzHqXvity"},"source":["Let's first look at the simple linear regression, where there is only one explanatory variable. Let's start with `MedInc`.\n","\n","**Q** Write down the simple linear regression model and define the variables.\n","\n","**A**\n","\n","_________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXOMb6oZvity"},"outputs":[],"source":["# Assign x and y accordingly\n","x = housing_df['________']\n","y = housing_df['________']"]},{"cell_type":"markdown","metadata":{"id":"j8TepcWRvity"},"source":["<u>The Numpy function `polyfit` gives the least squares polynomial fit. With the degree 1, it solves the simple linear regression problem.</u>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zMahgravity"},"outputs":[],"source":["# Find the estimates of w1 and w0\n","w1,w0 = np.polyfit(x,y,1)\n","\n","# print out the estimates\n","w1,w0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_xDMTDVvitz"},"outputs":[],"source":["# With the following new data points:\n","xnew = np.linspace(1,10,20)\n","\n","# Find the prediction yhat\n","ynew = xnew*w1 + w0"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"pLJ_i76_vitz"},"outputs":[],"source":["# use regplot in seaborn to plot x and y and the linear regression line\n","# on the same figure, plot the new estimated data points\n","sns.regplot(x=x,y=y)\n","plt.plot(xnew, ynew, 'rx')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4IZ04r1vvitz"},"source":["We will now calculate the performance of the simple linear model using the metrics\n","1. Mean absolute error (MAE)\n","2. Mean absolute percentage error (MAPE)\n","3. Mean squared error (MSE)\n","4. Root mean squared error (RMSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaWrL-5evitz"},"outputs":[],"source":["# First calculate the estimated y values using x and the estimated parameters\n","yhat = x*w1 + w0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeFUUVkNvit0"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error as mae, \\\n","mean_squared_error as mse, \\\n","mean_absolute_percentage_error as mape\n","\n","# Complete the following and print the results\n","#\n","mae_SLR = ________\n","mape_SLR =________\n","mse_SLR = ________\n","rmse_SLR = ________\n","print(\"MAE:\", mae_SLR)\n","print(\"MAPE:\", mape_SLR)\n","print(\"MSE:\", mse_SLR)\n","print(\"RMSE:\", rmse_SLR)"]},{"cell_type":"markdown","metadata":{"id":"S2UcIJ0Xvit0"},"source":["## Polynomial Regression"]},{"cell_type":"markdown","metadata":{"id":"CfHjjYz0vit0"},"source":["We will still use one feature, `MedInc`, but we will see if increasing the complexity of the model improves our prediction.\n","\n","Consider the 2nd degree polynomial model $$ y=w_{2}x^{2}+w_{1}x+w_{0}+\\varepsilon, $$ where $x$ is still the feature `MedInc`. We can estimate the parameters using the polyfit function in Numpy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcMTGK3Xvit0"},"outputs":[],"source":["# Estimate w0, w1, and w2 using the polyfit function\n","# then estimate the house prices from the same RM = xnew\n","w2,w1,w0 = np.________\n","\n","ynew = w2*xnew**2 + w1*xnew + w0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuOzbETZvit0"},"outputs":[],"source":["# Again use regplot in seaborn to plot x and y and\n","# on the same figure, plot the new estimated data points\n","sns.regplot(________)\n","plt.plot(xnew, ynew, 'rx')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"zbvvezpdvit1"},"outputs":[],"source":["# And print out the MAE, MSE, and RMSE\n","yhat = w2*x**2 + w1*x + w0\n","mae_PR = ________\n","mape_PR = ________\n","mse_PR = ________\n","rmse_PR = ________\n","print(\"MAE:\", mae_PR)\n","print(\"MAPE:\", mape_PR)\n","print(\"MSE:\", mse_PR)\n","print(\"RMSE:\", rmse_PR)"]},{"cell_type":"markdown","metadata":{"id":"kEKnlgyQvit1"},"source":["## Polynomial Regression with Interaction"]},{"cell_type":"markdown","metadata":{"id":"72L5izANvit1"},"source":["To illustrate the polynomial regression with interaction, take the 2 features `AveRooms` and `AveBedrms`.\n","\n","Let's take `MedInc` and `AveOccup` into a new DataFrame $X$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mb3Iy77Lvit1"},"outputs":[],"source":["X = housing_df[__________]\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S65N7I1Evit1"},"outputs":[],"source":["from sklearn.preprocessing import PolynomialFeatures\n","\n","poly_converter = _________(degree=2, include_bias=False)\n","poly_features = poly_converter.fit_transform(X)\n","poly_features\n","\n","# PolynomialFeatures will also take interaction terms into account\n","# With input [x1, x2], the degree-2 polynomial features are\n","# [1, x1, x2, x1^2, x1x2, x2^2] if include_bias=True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"In-1mSdfvit2"},"outputs":[],"source":["# Illustration\n","print(np.arange(1,4).reshape(1,3))\n","poly_converter.fit_transform(np.arange(1,4).reshape(1,3))"]},{"cell_type":"markdown","metadata":{"id":"9N4mN7ksvit2"},"source":["This time we'll use `sklearn` to solve for the coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU309Cwlvit2"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","model_PRI = _________()\n","model_PRI.fit(poly_features,y)\n","yhat = model_PRI.predict(poly_features)\n","mae_PRI = mae(y, yhat)\n","mape_PRI = mape(y, yhat)\n","mse_PRI = mse(y, yhat)\n","rmse_PRI = mse(y, yhat, squared=False)\n","print(\"MAE:\", mae_PRI)\n","print(\"MAPE:\", mape_PRI)\n","print(\"MSE:\", mse_PRI)\n","print(\"RMSE:\", rmse_PRI)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aORaugKbvit2"},"outputs":[],"source":["print(model_PRI.intercept_)\n","print(model_PRI.coef_)"]},{"cell_type":"markdown","metadata":{"id":"y7oVzH6Rvit2"},"source":["## Train-test split and model selection"]},{"cell_type":"markdown","metadata":{"id":"4lJQgkiOvit3"},"source":["Up to now we've only used the whole dataset to report the performance of our model. Of course this is unacceptable because the model is trained with this dataset. In this section we will do a data split to simulate an unknown data set to report the performance of our model.\n","\n","Referring to this page: [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html),\n","find out how you can implement a 80-20 training-test split below. Include all 8 features. Specify the `random_state` for reproducibility, say `random_state=123`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jj9W--Jvit3"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(housing_data.__________,#features\n","                                                    housing_data.__________,#target\n","                                                    test_size = 0.20,\n","                                                    random_state=123)\n","\n","# Alternative from a DataFrame:\n","X = housing_df.drop('medprice', axis=1)\n","y = housing_df['medprice']\n","X_train, X_test, y_train, y_test = train_test_split(__________,\n","                                                    __________,\n","                                                    test_size = 0.20,\n","                                                    random_state=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn4R6-xovit8"},"outputs":[],"source":["train_mapes = []\n","test_mapes = []\n","\n","for d in range(1,6):\n","    # new features with degree \"d\"\n","    poly_converter = PolynomialFeatures(degree=d,include_bias=False)\n","    poly_features = poly_converter.fit_transform(X)\n","\n","    # SPLIT THIS NEW POLY DATA SET\n","    X_train, X_test, y_train, y_test = train_test_split(poly_features, y,\n","                                                        test_size=0.2, random_state=123)\n","\n","    # TRAIN ON THIS NEW POLY SET\n","    model = LinearRegression()\n","    model.fit(X_train,y_train)\n","\n","    # PREDICT ON BOTH TRAIN AND TEST\n","    train_pred = model.predict(X_train)\n","    test_pred = model.predict(X_test)\n","\n","    # Calculate Errors\n","\n","    # Errors on Train Set\n","    train_MAPE = mape(y_train,train_pred)\n","\n","    # Errors on Test Set\n","    test_MAPE = mape(y_test,test_pred)\n","\n","    # Append errors to lists for plotting later\n","\n","    train_mapes.append(train_MAPE)\n","    test_mapes.append(test_MAPE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9zGpnFPvit8"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"ktsXcNWXvit9"},"outputs":[],"source":["plt.plot(range(1,6), train_mapes, label=\"Training MAPE\")\n","plt.plot(range(1,6), test_mapes, label=\"Testing MAPE\")\n","plt.ylabel(\"RMSE\")\n","plt.xlabel(\"Degree of polynomials\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"SMR5FmGuvit9"},"outputs":[],"source":["# Train the final model with the whole dataset\n","final_model_PR = LinearRegression(fit_intercept=False)\n","\n","poly_converter = PolynomialFeatures(degree=2,include_bias=False)\n","poly_features = poly_converter.fit_transform(housing_data.data)\n","\n","final_model_PR.fit(poly_features, housing_data.target)\n","\n","final_model_PR.intercept_\n","final_model_PR.coef_"]},{"cell_type":"markdown","metadata":{"id":"vF36uSJ7vit9"},"source":["Reference: [joblib](https://joblib.readthedocs.io/en/latest/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J140s8Kbvit9"},"outputs":[],"source":["# Saving the model\n","from joblib import dump, load\n","dump(final_model_PR, 'final_medhouseprice_model_PR.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KD45SvmXvit-"},"outputs":[],"source":["# Loading the model\n","loaded_final_PR_model = load('final_medhouseprice_model_PR.joblib')\n","loaded_final_PR_model.coef_"]},{"cell_type":"markdown","metadata":{"id":"EUyKW_AIvit-"},"source":["## Feature Scaling, Cross Validation , and Regularization"]},{"cell_type":"markdown","metadata":{"id":"16Ikxp85vit-"},"source":["It is necessary to standardize variables before using Lasso and Ridge Regression. In this section we will only take the Ridge Regression as an example."]},{"cell_type":"markdown","metadata":{"id":"R9kedh22vit-"},"source":["**Q**: Write the error model of the ridge regression. Why is it necessary to scale the features?\n","\n","**A**:_________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XBvuEu0vit-"},"outputs":[],"source":["X.shape, y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6GWB9gNvit_"},"outputs":[],"source":["# Do 80-20 split, specify the random_state=123\n","X_train, X_test, y_train, y_test = train_test_split(_________, _________,\n","                                                    _________,\n","                                                    _________)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OLMMNvvvit_"},"outputs":[],"source":["X_train.shape, X_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_qg1AqLvit_"},"outputs":[],"source":["# Run this cell to use StandardScaler from sklearn for the feature scaling\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5GAdwoPviuA"},"outputs":[],"source":["# We fit to the training data, not the whole data, to avoid data leakage\n","scaler.fit(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-JosL57viuA"},"outputs":[],"source":["# Scale the training and test features\n","scaled_X_train = _________._________(X_train)\n","scaled_X_test = _________._________(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTEjLG2hviuA"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","\n","# Optional\n","help(Ridge)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNq3U0kHviuA"},"outputs":[],"source":["# Alpha is a hyperparameter, later on we'll use cross validation to choose the optimal value\n","model_ridge = Ridge(alpha=0.1)\n","\n","# Fit the training set\n","model_ridge.fit(scaled_X_train, y_train)\n","\n","# Prediction on the test set\n","test_pred = _________._________(scaled_X_test)\n","\n","# Performance\n","print('MAPE: ', mape(y_test, test_pred))\n","print('RMSE: ', mse(y_test, test_pred, squared=False))\n"]},{"cell_type":"markdown","metadata":{"id":"gP4xsuFCviuB"},"source":["We want to decide $\\alpha$ by cross validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKug_afpviuB"},"outputs":[],"source":["from sklearn.linear_model import RidgeCV\n","\n","# Try changing alpha values\n","model_ridgecv = _________()\n","\n","#from sklearn.metrics import SCORERS\n","#SCORERS.keys()\n","_________.fit(_________, _________)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7_7dYUqviuB"},"outputs":[],"source":["model_ridgecv.alpha_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d25AxpJsviuB"},"outputs":[],"source":["# Fit the training set\n","model_ridgecv.fit(scaled_X_train, y_train)\n","\n","# Prediction on the test set\n","test_pred = model_ridgecv.predict(scaled_X_test)\n","\n","# Print the MSE\n","print('MAPE: ', mape(y_test, test_pred))\n","print('RMSE: ', mse(y_test, test_pred, squared=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFJOhPvwviuC"},"outputs":[],"source":["model_ridgecv.coef_"]},{"cell_type":"markdown","source":["Now, instead of RidgeCV, use LassoCV to find the optimum alpha for Lasso regularized model and print out the MAPE and RMSE."],"metadata":{"id":"PEsiy7cto0NM"}}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"245.76px"},"toc_section_display":true,"toc_window_display":true},"colab":{"provenance":[{"file_id":"1pqtCe5KFOtroVOpNCg_cnJOq-sf2Cksc","timestamp":1687293999238}]}},"nbformat":4,"nbformat_minor":0}