{"cells":[{"cell_type":"markdown","metadata":{"id":"WulBWFVBJaxj"},"source":["# Data Science 1 - Tutorial 5.3 - Classification"]},{"cell_type":"markdown","metadata":{"id":"f2mPKuQbJaxn"},"source":["## The Breast Cancer Wisconsin Dataset"]},{"cell_type":"markdown","metadata":{"id":"u6BaDViDJaxn"},"source":["For this exercise we will use the [the breast cancer wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kiykcjMJaxo"},"outputs":[],"source":["# Go to the linked page and find out how to import the dataset\n","from sklearn.datasets import load_breast_cancer\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"olkK0f1sJaxq"},"source":["### Understanding the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVL9PWW9Jaxq"},"outputs":[],"source":["# Load the dataset\n","bc_data = __________\n","\n","# Print the desription\n","print(bc_data.__________)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLBAiRv6Jaxq"},"outputs":[],"source":["# Save the dataset into a DataFrame\n","# As the column names, take feature_names for the explanatory variables\n","# and \"type\" for the response\n","\n","# __________\n","\n","bc_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"iLzLrf_aJaxr"},"outputs":[],"source":["# We've seen a pairplot before, here we introduce a heatmap\n","# to display all the pairwise correlation values\n","plt.figure(figsize=(20,20))\n","\n","# Use heatmap from seaborn\n","__________(bc_df._____, #correlation values of bc_df\n","           annot=True);\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrgbGrjgJaxr"},"outputs":[],"source":["# Map the values of \"type\" in bc_df into: 0:\"malignant\", 1:\"benign\"\n","\n","bc_df['type'] = bc_df._____._____(__________)\n","\n","# Display the value counts for each malignant and benign class\n","bc_df['type'].__________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGeuH3wmJaxs"},"outputs":[],"source":["# This is just an example of another type of display, catplot\n","# Run this and optionally, find out how you can change the kind of the plot\n","sns.catplot(x=\"worst perimeter\", y=\"type\", data=bc_df,# kind=\"box\",\n","           height=3, aspect=2);"]},{"cell_type":"markdown","metadata":{"id":"eAvCJkxOJaxt"},"source":["### Splitting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwrfLQymJaxt"},"outputs":[],"source":["# Perform an 80/20 train-test-split\n","\n","from __________ import __________\n","\n","X_train, X_test, y_train, y_test = train_test_split(__________, ___________, # Use the DataFrame\n","                                                    stratify=__________, # Stratify the split!\n","                                                    random_state=123)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"63aS8tjvJaxt"},"outputs":[],"source":["# Let's display the stratified data split\n","# Simply run this\n","fig, ax =plt.subplots(1,2, figsize=(8,3)) #, sharey=True)\n","sns.countplot(x=y_train, ax=ax[0], order=[\"malignant\", \"benign\"])\n","sns.countplot(x=y_test, ax=ax[1], order=[\"malignant\", \"benign\"]);"]},{"cell_type":"markdown","metadata":{"id":"meJL_yAhJaxu"},"source":["## Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-_UgAIoJaxu"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","# Complete the following\n","\n","# Instantiate the tree\n","model_tree = __________\n","\n","# Fit the tree to the training set\n","__________\n","\n","print('Training accuracy: ', model_tree.score(X_train, y_train))"]},{"cell_type":"markdown","metadata":{"id":"SUXHKKfjJaxu"},"source":["### Evaluation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cY62wBRXJaxv"},"outputs":[],"source":["from sklearn.metrics import classification_report, plot_confusion_matrix\n","\n","# Using the fitted model_tree, predict the outcomes from the test set\n","pred_tree = __________\n","\n","# Using the imported metrics, print out the reports\n","# classification report\n","__________\n","\n","# plot the confusion matrix\n","__________"]},{"cell_type":"markdown","metadata":{"id":"9761ox3nJaxv"},"source":["### Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1X_56S9Jaxv"},"outputs":[],"source":["# How can we print out the feature importances?\n","\n","# print(model_tree.__________)\n","\n","# These numbers correspond to the columns of the features\n","print(X_train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WN9ifLnrJaxw"},"outputs":[],"source":["# We can collect the feature importances in a DataFrame and sort the values for easier exploration\n","# Fill in the following to display the sorted feature importance values\n","\n","pd.DataFrame(__________, # set column names as the index\n","             __________, # feature importance values\n","             __________ # column name for the feature importance\n","            ).__________ # sort ascending"]},{"cell_type":"markdown","metadata":{"id":"kg5QO0jzJaxw"},"source":["### Displaying the Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"6Q3Z3d6AJaxw"},"outputs":[],"source":["# Run this cell\n","from sklearn.tree import plot_tree\n","\n","plt.figure(figsize=(20,10), dpi=300)\n","plot_tree(model_tree, feature_names=X_train.columns);#, filled=True);"]},{"cell_type":"markdown","metadata":{"id":"hf1g3FNyJaxx"},"source":["### Changing some hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"FDW8rgO9Jaxx"},"source":["To see the effects of changing some of the parameters, let's create a function to simplify our reporting task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v810RUdgJaxx"},"outputs":[],"source":["# Complete the following\n","def tree_report(model):\n","    test_pred = model.__________(__________) # prediction on the test set\n","\n","    # print the classification report\n","    print(__________)\n","\n","    # plot the confusion matrix\n","    __________\n","\n","    # plot the resulting tree\n","    plt.figure(figsize=(20,10), dpi=300)\n","    __________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8L-NdvUJaxy"},"outputs":[],"source":["# Initiate a pruned tree with depth=3\n","model_pruned = DecisionTreeClassifier(__________,\n","                                      random_state=123)\n","\n","# Fit the pruned tree to the training set\n","__________\n","\n","# Generate the report\n","__________\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxbF_hwLJaxy"},"outputs":[],"source":["# Now try changing other hyperparameters and report the performance."]},{"cell_type":"markdown","metadata":{"id":"F0UGSWF6Jaxy"},"source":["## Random Forests"]},{"cell_type":"markdown","metadata":{"id":"oQIGfXD9Jaxy"},"source":["1. What might be the motivation of random forest (i.e. what are the shortcomings of using a single decision tree)?"]},{"cell_type":"markdown","metadata":{"id":"1DznFw2VJaxz"},"source":["**A**:"]},{"cell_type":"markdown","metadata":{"id":"qw7S-1LPJaxz"},"source":["2. List two hyperparameters of random forests?"]},{"cell_type":"markdown","metadata":{"id":"OdDjvW-nJaxz"},"source":["**A**:"]},{"cell_type":"markdown","metadata":{"id":"XHW-8ypsJax0"},"source":["3. Explain bagging in random forest."]},{"cell_type":"markdown","metadata":{"id":"qySs-uSbJax0"},"source":["**A**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP7VobbSJax0"},"outputs":[],"source":["# Complete the following\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","model_rf = RandomForestClassifier(___________, # Set the number of trees to 20\n","                                 random_state=123)\n","\n","# Fit the model to the training set\n","__________\n","\n","# Predict from the test set\n","_________\n","\n","# Print the classification report and plot the confusion matrix\n","__________"]},{"cell_type":"markdown","metadata":{"id":"5_jE7vwnJax1"},"source":["### Using Cross Validation for Random Forest"]},{"cell_type":"markdown","metadata":{"id":"aLWbuaDaJax1"},"source":["We'll use grid search to decide the two hyperparameters. Later on in the SVM section, we will again use GridSearchCV."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8WduhcsQJax1"},"outputs":[],"source":["# Run this cell\n","from sklearn.model_selection import GridSearchCV\n","\n","n_estimators = [32, 64, 96, 128, 160]\n","max_features = [4,5,6,8]\n","param_grid = {'n_estimators': n_estimators,\n","              'max_features': max_features}\n","\n","model_rf = RandomForestClassifier()\n","grid = GridSearchCV(model_rf, param_grid)\n","grid.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltOR8ZwVJax1"},"outputs":[],"source":["# Print the best parameters\n","grid.__________"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQpz0woFJayE"},"outputs":[],"source":["# Fit the model using the best chosen parameters and report the performance\n","model_rfcv = __________\n","__________"]},{"cell_type":"markdown","metadata":{"id":"Zd0UF5y1JayE"},"source":["### Error vs. number of trees"]},{"cell_type":"markdown","metadata":{"id":"_4L28tDrJayE"},"source":["We'll now use another technique to choose the number of trees, namely, to plot the errors and number of misclassifications against the number of trees."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HSzSRQJJayF"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","errors = []\n","misclassifications = []\n","\n","for n in range(1,200,5):\n","    # Instantiate, fit, predict\n","    model_rf = __________ # set max_features=6\n","    model_rf.__________\n","    test_pred = __________\n","\n","    e = 1-accuracy_score(y_test, test_pred)\n","    missed = __________ # How to count the misclassified outcomes?\n","\n","    errors.append(e)\n","    misclassifications.append(missed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_b110-DJayF"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n","# Plot errors and misclassifications side by side\n","__________"]},{"cell_type":"markdown","metadata":{"id":"px0lxa-HJayG"},"source":["## SVM"]},{"cell_type":"markdown","metadata":{"id":"PG97YWqgJayG"},"source":["We'll take a subset of the breast cancer dataframe to illustrate the separating hyperplane."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"577gEcLCJayG"},"outputs":[],"source":["# Take the two columns which have the highest feature importance values (Subsection 2.2 above) and the target column\n","df = __________\n","\n","\n","sns.scatterplot(x=_________, y=_________, # The order doesn't matter here\n","                hue=__________, # Differentiate the color based on the outcomes\n","                data=df);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPPzuGdMJayG"},"outputs":[],"source":["from sklearn.svm import SVC\n","#help(SVC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvCXNpYPJayH"},"outputs":[],"source":["# Run this helper function\n","def plot_svm(model,X,y):\n","\n","    X = X.values\n","\n","    # Scatter Plot\n","    plt.scatter(X[:, 0], X[:, 1],\n","                c=y, s=30,cmap='seismic')\n","\n","\n","    # plot the decision function\n","    ax = plt.gca()\n","    xlim = ax.get_xlim()\n","    ylim = ax.get_ylim()\n","\n","    # create grid to evaluate model\n","    xx = np.linspace(xlim[0], xlim[1], 30)\n","    yy = np.linspace(ylim[0], ylim[1], 30)\n","    YY, XX = np.meshgrid(yy, xx)\n","    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n","    Z = model.decision_function(xy).reshape(XX.shape)\n","\n","    # plot decision boundary and margins\n","    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n","               linestyles=['--', '-', '--'])\n","    # plot support vectors\n","    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n","               linewidth=1, facecolors='none', edgecolors='k')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBG7ABP8JayH"},"outputs":[],"source":["# Also run this\n","X = df.drop('type', axis=1)\n","y = bc_data.target\n","\n","model_svm = SVC(kernel='linear')\n","model_svm.fit(X,y)\n","\n","plt.figure(figsize=(10,5))\n","plot_svm(model_svm, X, y)"]},{"cell_type":"markdown","metadata":{"id":"R_OBR3B-JayI"},"source":["Try it yourself:\n","\n","1. Research: What is kernel in SVM, what is radial basis function?\n","2. Now plot the decision boundary with the rbf kernel (Simply remove the argument `kernel='linear'` because `rbf` is the default value). Try changing the values of `C` and `gamma`.\n","3. Use `GridSearchCV` as shown in Section 3.1 to find the best among these hyperparameters:\n","`'C':[0.01, 0.1, 1, 10]`, `'kernel':['linear','rbf']`."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"165px"},"toc_section_display":true,"toc_window_display":true},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}